{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd33bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"doesn't work at the moment!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d015c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe054cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.plotting_tools.Bins import bins\n",
    "from ROOT import TH1F\n",
    "import array\n",
    "import subprocess\n",
    "import uncertainties as unc\n",
    "from uncertainties import unumpy\n",
    "\n",
    "from scipy.integrate import quad\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ec163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting_tools.utils import calc_bin_widths, calc_bin_centers\n",
    "from src.plotting_tools.SysHist import SysHist\n",
    "from src.data_tools.StackPlotter import get_stack_plotter\n",
    "from src.plotting_tools.utils import rebin_np, rebin_integrate\n",
    "from src.plotting_tools.cms_format import cms_format_fig, cms_style\n",
    "from src.plotting_tools.Bins import Bins\n",
    "from src.assets.lumi import lumi_dict\n",
    "from src.assets.shape_scaling import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074aeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cms_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hist(values, errors, bin_edges, *args):\n",
    "    hpx    = TH1F(*args, len(bin_edges)-1, array.array('d', bin_edges))\n",
    "    for i, (x,e) in enumerate(zip(values,errors)):\n",
    "        hpx.SetBinContent(i, x) \n",
    "        hpx.SetBinError(i, e) \n",
    "    return hpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_overflow(arr, top=0, bottom=0):\n",
    "    return   np.concatenate([[bottom],arr,[top]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://cms-analysis.github.io/HiggsAnalysis-CombinedLimit/part2/settinguptheanalysis/\n",
    "# https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/102x/data/tutorials/shapes/simple-shapes-df_input.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.assets.output_dir import output_dir\n",
    "outdir = output_dir\n",
    "era = '2016'\n",
    "lumi_fraction = lumi_dict[str(era)]/lumi_dict['201X']\n",
    "#rootfname = '{outdir}/combine_data/{era}/{era}_shapes_df_input.root'.format(outdir=outdir, era=era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if era=='2016':\n",
    "    lumi=1.025\n",
    "    uncorr = 1.01\n",
    "    corr_1 = 1.006\n",
    "    corr_2 = 1.0\n",
    "    lumi_str = f'''\n",
    "lumi_uncorr_{era} lnN -      {uncorr}\n",
    "lumi_corr1 lnN -      {corr_1}\n",
    "'''\n",
    "if era=='2017':\n",
    "    lumi=1.023\n",
    "    uncorr = 1.02\n",
    "    corr_1 = 1.009\n",
    "    corr_2 = 1.006\n",
    "    lumi_str = f'''\n",
    "lumi_uncorr_{era} lnN -      {uncorr}\n",
    "lumi_corr1 lnN -      {corr_1}\n",
    "lumi_corr2  lnN -      {corr_2}\n",
    "'''\n",
    "if era=='2018':\n",
    "    lumi=1.025\n",
    "    uncorr = 1.015\n",
    "    corr_1 = 1.02\n",
    "    corr_2 = 1.002\n",
    "    lumi_str = f'''\n",
    "lumi_uncorr_{era} lnN -      {uncorr}\n",
    "lumi_corr1 lnN -      {corr_1}\n",
    "lumi_corr2  lnN -      {corr_2}\n",
    "'''\n",
    "print(lumi_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = get_stack_plotter(outdir, era)\n",
    "data_dict = {}\n",
    "data_dict['SR1'] = sp.make_data_hist('DiLepMass','SR1', blinded=False)\n",
    "data_dict['SR2'] = sp.make_data_hist('DiLepMass','SR2', blinded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/data/{}_bff_interp_dbs_norm.pkl'.format(outdir, era), 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "masses = data.mass.unique()\n",
    "masses = masses[masses<355]\n",
    "full_masses= True\n",
    "masses = masses if full_masses else [125, 150, 175, 200, 250, 300, 350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005428bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "outname=\"{}/abcd/abcd_dict_data_{}_ismc0.pkl\".format(outdir, era)\n",
    "with open(outname,'rb') as f:\n",
    "    abcd = pkl.load(f)\n",
    "    \n",
    "outname=\"{}/abcd/ABCD_closure_unc.pkl\".format(outdir, era)\n",
    "with open(outname, 'rb') as f:\n",
    "    uncertainty_dict= pkl.load(f)\n",
    "    \n",
    "    \n",
    "    print(pd.DataFrame(uncertainty_dict))\n",
    "uncertainty_dict = {k:v+1 for k,v in uncertainty_dict[int(era)].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Make acceptance details\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12cede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accpt_df = pd.read_csv('/eos/cms/store/group/phys_exotica/bffZprime/assets_june_23'+\"/data_gen_b_s/summary_df.csv\")\n",
    "accpt_df\n",
    "\n",
    "isrfsr = abs((accpt_df['Weight_ISRFSR_Up']-accpt_df['Weight_ISRFSR_Down']))/(accpt_df['acceptance']*2)\n",
    "\n",
    "pdf = abs(accpt_df['Weight_PDF_Up']-accpt_df['Weight_PDF_Down'])/(accpt_df['acceptance']*2)\n",
    "\n",
    "min(isrfsr), max(isrfsr), np.mean(isrfsr), min(pdf), max(pdf), np.mean(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a13034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, RationalQuadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPR_fit_and_predict(x,y,std, xp, plot=False, length_scale_bounds=(100,10000),alpha=1e-1):\n",
    "    kernel = 1 * RBF(length_scale=100, length_scale_bounds=length_scale_bounds)\n",
    "    gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9, random_state=0, alpha=alpha, normalize_y=True)\n",
    "    gaussian_process.fit(x, y)\n",
    "    #predict\n",
    "    mean_prediction, std_prediction = gaussian_process.predict(xp, return_std=True)\n",
    "    mean_prediction = mean_prediction.reshape(-1)\n",
    "    if plot:\n",
    "        plot.errorbar(x, y, yerr=std, label='Data')\n",
    "        #gpr:\n",
    "        plot.plot(xp, mean_prediction, color='orange', label='GPR Fit')\n",
    "        plot.fill_between(xp.reshape(-1),  mean_prediction+std_prediction, mean_prediction-std_prediction, alpha=.5, color='orange')\n",
    "    return  mean_prediction, std_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_accpt_df(reg, sig_type):\n",
    "    tadf = accpt_df[(accpt_df.reg==reg) & (accpt_df.type==sig_type)]\n",
    "\n",
    "    #accp\n",
    "    fig, ax = plt.subplots()\n",
    "    cms_format_fig(era, ax, \"\\emph{Simulation}\")\n",
    "    ax.set_ylabel('Acceptance')\n",
    "    ax.set_xlabel('$m_{Z\\prime}$ [GeV]')\n",
    "    ax.legend(title=f'{reg} {sig_type}')\n",
    "    accep, stat = GPR_fit_and_predict(tadf.mass.to_numpy().reshape(-1, 1), tadf.acceptance.to_numpy().reshape(-1, 1), tadf.statistical.to_numpy(),\n",
    "       masses.reshape(-1,1), plot=ax, length_scale_bounds=(100,10000), alpha=tadf.statistical.to_numpy()*1e1)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cms_format_fig(era, ax, \"\\emph{Simulation}\")\n",
    "    ax.set_ylabel('ISR/FSR Weight')\n",
    "    ax.set_xlabel('$m_{Z\\prime}$ [GeV]')\n",
    "    ax.legend(title=f'{reg} {sig_type}')\n",
    "    Weight_ISRFSR_Up, _  = GPR_fit_and_predict(tadf.mass.to_numpy().reshape(-1, 1), tadf.Weight_ISRFSR_Up.to_numpy().reshape(-1, 1), 0*tadf.statistical.to_numpy(),\n",
    "       masses.reshape(-1,1), plot=ax, length_scale_bounds=(100,10000), alpha=2e-1)\n",
    "    \n",
    "    Weight_ISRFSR_Down, _  = GPR_fit_and_predict(tadf.mass.to_numpy().reshape(-1, 1), tadf.Weight_ISRFSR_Down.to_numpy().reshape(-1, 1), 0*tadf.statistical.to_numpy(),\n",
    "       masses.reshape(-1,1), plot=ax, length_scale_bounds=(100,10000), alpha=2e-1)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cms_format_fig(era, ax, \"\\emph{Simulation}\")\n",
    "    ax.set_ylabel('PDF Weight')\n",
    "    ax.set_xlabel('$m_{Z\\prime}$ [GeV]')\n",
    "    ax.legend(title=f'{reg} {sig_type}')\n",
    "    Weight_PDF_Up, _  = GPR_fit_and_predict(tadf.mass.to_numpy().reshape(-1, 1), tadf.Weight_PDF_Up.to_numpy().reshape(-1, 1), 0*tadf.statistical.to_numpy(),\n",
    "       masses.reshape(-1,1), plot=ax, length_scale_bounds=(100,10000), alpha=2e-1)\n",
    "\n",
    "    Weight_PDF_Down, _  = GPR_fit_and_predict(tadf.mass.to_numpy().reshape(-1, 1), tadf.Weight_PDF_Down.to_numpy().reshape(-1, 1), 0*tadf.statistical.to_numpy(),\n",
    "       masses.reshape(-1,1), plot=ax, length_scale_bounds=(100,10000), alpha=2e-1)\n",
    "    \n",
    "    #make df\n",
    "    df_list = []\n",
    "    for mass, a, s, ISRFSR_Up, ISRFSR_Down, PDF_up, PDF_Down in zip(masses, accep, stat, Weight_ISRFSR_Up, Weight_ISRFSR_Down, Weight_PDF_Up, Weight_PDF_Down):\n",
    "        df_list.append({\"mass\": mass, \"reg\": reg, \"type\": sig_type, \n",
    "                        \"acceptance\": a, 'statistical': s, \n",
    "                        \"Weight_ISRFSR_Up\": ISRFSR_Up, \"Weight_ISRFSR_Down\": ISRFSR_Down,\n",
    "                        \"Weight_PDF_Up\": PDF_up, \"Weight_PDF_Down\": PDF_Down})\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947c1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for reg in ['SR1', 'SR2']:\n",
    "    for sig_type in accpt_df.type.unique():\n",
    "        df_list+= extend_accpt_df(reg, sig_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba989d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "accpt_df_interpolated = pd.DataFrame(df_list)\n",
    "accpt_df_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da25657",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## define template\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''#higgs combine tool shape analysis card for z'to mumu 1 jet\n",
    "#https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/102x/data/tutorials/shapes/simple-shapes-df.txt\n",
    "-------------------------\n",
    "\n",
    "imax 1  number of channels                                      #1 Jet\n",
    "jmax 1  number of backgrounds -1                                    #following AN2015_207_v5, not sure why the -1 is there?\n",
    "kmax *  number of nuisance parameters (sources of systematic uncertainties)\n",
    "\n",
    "-------------------------\n",
    "\n",
    "bin       {reg}_{era}_{binCount}\n",
    "observation   {obs}\n",
    "\n",
    "-------------------------\n",
    "\n",
    "bin       {reg}_{era}_{binCount}      {reg}_{era}_{binCount}\n",
    "process     ABCD_{reg}_{era}    sig_{reg}_{era}_{mass}_{dbs}\n",
    "process     1     -1\n",
    "rate      {abcd_count}   {sig_count}\n",
    "\n",
    "-------------------------\n",
    "back_fit_{era}_{binCount} lnN {back_fit}    -    \n",
    "Closure_{era}_{binCount} lnN  {back_closure}   -  \n",
    "jer_{era}   lnN -      {jer}\n",
    "jes_{era}   lnN -      {jes}\n",
    "roch_{era}   lnN -      {roch}\n",
    "HEM_{era}   lnN -      {HEM}\n",
    "btagCorr   lnN -      {btagCorr}\n",
    "btagUncorr_{era}   lnN -      {btagUncorr}\n",
    "elSF_{era}   lnN -      {el}\n",
    "ISRFSR_{era}   lnN -      {ISRFSR}\n",
    "Muon_{era}   lnN -      {Muon}\n",
    "trigger_{era}   lnN -      {trigger}\n",
    "pdf_{era}   lnN -      {pdf}\n",
    "puid_{era}   lnN -      {puid}\n",
    "pu   lnN -      {pu}'''\n",
    "template += lumi_str\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fa483",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## rebin_signals\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283540e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin_integrate(x,y, xnew_binedges, keep_norm=False, usequad=False, straight_interpolate = False, sample_density=100):\n",
    "    from scipy.interpolate import interp1d\n",
    "    from scipy.integrate import quad\n",
    "    f = interp1d(x,y, fill_value=0, bounds_error=False)\n",
    "     \n",
    "    centers = np.array(Bins(bin_edges).calc_bin_centers())\n",
    "    widths = np.array(Bins(bin_edges).calc_bin_widths())\n",
    "    if usequad:\n",
    "        ynew_int = []\n",
    "        for i, center in enumerate(centers):\n",
    "            integral = quad(f, xnew_binedges[i], xnew_binedges[i+1])[0]\n",
    "            ynew_int.append(integral)\n",
    "        ynew_int = np.array(ynew_int)\n",
    "    elif straight_interpolate:\n",
    "        ynew_int = f(centers)*widths\n",
    "    else:\n",
    "        ynew_int = []\n",
    "        for i, (center, width) in enumerate(zip(centers, widths)):\n",
    "            sample_points = np.linspace(xnew_binedges[i], xnew_binedges[i+1],int(sample_density*width))\n",
    "            integral = f(sample_points).sum()/sample_density\n",
    "            ynew_int.append(integral)\n",
    "        ynew_int = np.array(ynew_int)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if keep_norm:  \n",
    "        y_total = y[(x>=np.min(bin_edges)) & (x<=np.max(bin_edges))].sum()\n",
    "        return centers, ynew_int/ynew_int.sum()*y_total\n",
    "    return centers, ynew_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d9933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rebinned = []\n",
    "bin_edges = abcd['SR1']['bins']\n",
    "start = perf_counter()\n",
    "for i, row in data.iterrows():\n",
    "    \n",
    "    x, y = row.x, row.y\n",
    "    xnew, ynew = rebin_integrate(x, y, bin_edges, keep_norm=True, sample_density=2)\n",
    "    rebinned.append(ynew)\n",
    "    if i%1000==10: \n",
    "        end = perf_counter()\n",
    "        elapsed_time_min = (end-start)/60\n",
    "        time_per = elapsed_time_min/i\n",
    "        nremaining = len(data)-i\n",
    "        time_remaining = time_per*nremaining\n",
    "        print(\"ratio done:{:.2f} elapsed minutes: {:.1f} est time remaining: {:.1f}\".format(i/len(data), elapsed_time_min, time_remaining))\n",
    "        \n",
    "end = perf_counter()\n",
    "print(end-start)\n",
    "data['rebinned'] = rebinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_bin(reg, mass, dbs, sys, nBin):\n",
    "    tdf =  data[(data.reg==reg) & (data.mass==mass) & (data.dbs==dbs) & (data.sys==sys)]\n",
    "    assert tdf.shape[0]==1, \"more than length one\"\n",
    "    row =tdf.iloc[0]\n",
    "    x, y = row.x, row.y\n",
    "    y_prime = rebin_np(x, bin_edges, y) \n",
    "    return y_prime[nBin]\n",
    "\n",
    "def get_sig_bins(reg, mass, dbs, sys, nBin, nom):\n",
    "    nom = nom+.001\n",
    "    down = get_sig_bin(reg, mass, dbs, sys.format(\"Down\"), nBin)\n",
    "    up = get_sig_bin(reg, mass, dbs, sys.format(\"Up\"), nBin)\n",
    "    x = [down/nom, up/nom]\n",
    "    if x == [0,0]: x = [1,1]\n",
    "    if (x[0]==x[1]): string = \"{:.2f}\".format(x[0])\n",
    "    else: string =  \"{:.2f}/{:.2f}\".format(*x)\n",
    "    if string == \"1.00/1.00\": string = \"1.00\"\n",
    "    return string\n",
    "\n",
    "def get_norm_bin(reg, mass, dbs, sys, norm):\n",
    "    tdf =  data[(data.reg==reg) & (data.mass==mass) & (data.dbs==dbs) & (data.sys==sys)]\n",
    "    assert tdf.shape[0]==1, \"more than length one\"\n",
    "    row =tdf.iloc[0]\n",
    "    return norm/row.y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5516730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_bin(reg, mass, dbs, sys, nBin):\n",
    "    tdf =  data[(data.reg==reg) & (data.mass==mass) & (data.dbs==dbs) & (data.sys==sys)]\n",
    "    assert tdf.shape[0]==1, \"more than length one\"\n",
    "    row =tdf.iloc[0]\n",
    "    y_prime = row.rebinned[nBin]\n",
    "    return y_prime\n",
    "\n",
    "def get_sig_bins(reg, mass, dbs, sys, nBin, nom):\n",
    "    nom = nom+.001\n",
    "    down = get_sig_bin(reg, mass, dbs, sys.format(\"Down\"), nBin)\n",
    "    up = get_sig_bin(reg, mass, dbs, sys.format(\"Up\"), nBin)\n",
    "    x = [down/nom, up/nom]\n",
    "    if x == [0,0]: x = [1,1]\n",
    "    if (x[0]==x[1]): string = \"{:.2f}\".format(x[0])\n",
    "    else: string =  \"{:.2f}/{:.2f}\".format(*x)\n",
    "    if string == \"1.00/1.00\": string = \"1.00\"\n",
    "    return string\n",
    "\n",
    "def get_norm_bin(reg, mass, dbs, sys, norm):\n",
    "    tdf =  data[(data.reg==reg) & (data.mass==mass) & (data.dbs==dbs) & (data.sys==sys)]\n",
    "    assert tdf.shape[0]==1, \"more than length one\"\n",
    "    row =tdf.iloc[0]\n",
    "    return norm/row.y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806142f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bin_card(reg, mass, dbs, nBin, norm, stat_err, ISRFSR=\"0.97/1.03\", pdf=\"1.0\", verbose=False):\n",
    "    bin_edges = abcd[reg]['bins']\n",
    "    nom = abcd[reg]['nom'][nBin]\n",
    "    std = abcd[reg]['std'][nBin]\n",
    "    target = Bins(bin_edges).calc_bin_centers()[nBin]\n",
    "    #use new get value function to ensure matching data value\n",
    "    obs = data_dict[reg].get_value_at(target)[0]\n",
    "    if verbose: print(obs, nom)\n",
    "    #blinded\n",
    "    obs = obs\n",
    "    #signal \n",
    "    sig_nom = get_sig_bin(reg, mass, dbs, 'nom', nBin)\n",
    "    jes = get_sig_bins(reg, mass, dbs, 'Reg_jet_jesTotal{}_muon_corrected_pt_ele_pt', nBin, sig_nom)\n",
    "    roch = get_sig_bins(reg, mass, dbs, 'Reg_jet_nom_muon_corrected{}_pt_ele_pt', nBin, sig_nom)\n",
    "    jer = get_sig_bins(reg, mass, dbs, 'Reg_jet_jer{}_muon_corrected_pt_ele_pt', nBin, sig_nom)\n",
    "    pu = get_sig_bins(reg, mass, dbs, 'Weight_Pu{}', nBin, sig_nom)\n",
    "    btagCorr = get_sig_bins(reg, mass, dbs, 'Weight_BTagCorr{}', nBin, sig_nom)\n",
    "    btagUncorr = get_sig_bins(reg, mass, dbs, 'Weight_BTagUncorr{}', nBin, sig_nom)\n",
    "    puid = get_sig_bins(reg, mass, dbs, 'Weight_PUID{}', nBin, sig_nom)\n",
    "    #pdf = get_sig_bins(reg, mass, dbs, 'Weight_PDF_{}', nBin, sig_nom)\\\n",
    "    pdf = pdf\n",
    "    #fixed 2% width\n",
    "    ISRFSR  = ISRFSR #= get_sig_bins(reg, mass, dbs, 'Weight_ISRFSR_{}', nBin, sig_nom)\n",
    "    muon = get_sig_bins(reg, mass, dbs, 'Weight_MuonSF{}', nBin, sig_nom)\n",
    "    el = get_sig_bins(reg, mass, dbs, 'Weight_ElectronSF{}', nBin, sig_nom)\n",
    "    trigger = get_sig_bins(reg, mass, dbs, 'Weight_MuonTrigger{}', nBin, sig_nom)\n",
    "    try:\n",
    "        HEM = get_sig_bins(reg, mass, dbs, \"Reg_jet_jesHEMIssue{}_muon_corrected_pt_ele_pt\", nBin, sig_nom)\n",
    "    except:\n",
    "        HEM = \"1.00\"\n",
    "        \n",
    "    \n",
    "    norm_factor = get_norm_bin(reg, mass, dbs, 'nom', norm)\n",
    "    sig_nom = sig_nom*norm_factor\n",
    "\n",
    "    value_dict = {\n",
    "                \"era\": era,\n",
    "                \"reg\": reg,\n",
    "                \"binCount\": nBin,\n",
    "                \"lumi\": lumi, \n",
    "                \"mass\": mass, \n",
    "                \"dbs\": dbs,\n",
    "                \"obs\": \"{:.2f}\".format(obs),\n",
    "        \n",
    "                \"abcd_count\": \"{:.2f}\".format(nom),\n",
    "                \"back_fit\": \"{:.2f}\".format((nom+std)/nom),\n",
    "                \"back_closure\": \"{:.2f}\".format(uncertainty_dict[reg]),\n",
    "        \n",
    "                \"sig_count\": \"{:.2f}\".format(sig_nom),\n",
    "                \"jer\": jer,\n",
    "                \"jes\": jes,\n",
    "                \"roch\": roch,\n",
    "                \"HEM\": HEM,\n",
    "                \"btagCorr\": btagCorr,\n",
    "                \"btagUncorr\": btagUncorr,\n",
    "                \"el\": el,\n",
    "                \"ISRFSR\": ISRFSR,\n",
    "                \"Muon\": muon,\n",
    "                \"trigger\": trigger,\n",
    "                \"pdf\": pdf,\n",
    "                \"puid\": puid,\n",
    "                \"pu\": pu,\n",
    "                \"stat\": \"{:.2f}\".format(1+stat_err/norm),\n",
    "                 }\n",
    "    return template.format(**value_dict), value_dict, sig_nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e90ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/afs/cern.ch/work/r/rymuelle/public/nanoAODzPrime/CMSSW_12_1_0/src/bff_plotting_code_v3/exo-datacards/EXO-22-006/combine_data/model_ind'\n",
    "path = '{}/combine_data/model_ind'.format(outdir)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c16938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ddefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers = calc_bin_centers(abcd['SR1']['bins'])\n",
    "reg, mass, dbs, fs_type = 'SR1', 250, 0.04, '1b'\n",
    "bin_edges = abcd[reg]['bins']\n",
    "\n",
    "def make_card(reg, mass, dbs, fs_type):\n",
    "    combine_str = 'combineCards.py '\n",
    "    \n",
    "    signal = '{}_{}_{}_{}'.format(reg, mass, str(dbs).replace('.', 'p'), fs_type.replace('(','-').replace(')','-'))\n",
    "    path_prime = '{}/{}/{}'.format(path, era, signal)\n",
    "\n",
    "    acceptance_row = accpt_df_interpolated[(accpt_df_interpolated.reg==reg) & (accpt_df_interpolated.mass==mass) & (accpt_df_interpolated.type==fs_type)]\n",
    "    assert acceptance_row.shape[0] == 1, acceptance_row\n",
    "    acceptance = acceptance_row.iloc[0]['acceptance']\n",
    "    stat_error = acceptance_row.iloc[0]['statistical']\n",
    "    norm = scale*lumi_fraction*acceptance\n",
    "    \n",
    "    isrfsr = get_sys(acceptance_row.iloc[0], \"Weight_ISRFSR\")\n",
    "    pdf = get_sys(acceptance_row.iloc[0], \"Weight_PDF\")\n",
    "    os.makedirs(path_prime, exist_ok=True)\n",
    "    \n",
    "    signal_total  = 0\n",
    "    for i in range(len(bin_centers)):\n",
    "        # don't count 0 sig bins for speed\n",
    "        sig_count = get_sig_bin(reg, mass, dbs, 'nom', i)\n",
    "        if sig_count==0: continue\n",
    "        \n",
    "        template_filled, value_dict, sig_nom = make_bin_card(reg, mass, dbs, i, norm, stat_error, ISRFSR=isrfsr, pdf=pdf)\n",
    "        if float(value_dict['sig_count']) == 0: continue\n",
    "        signal_total+=float(value_dict['sig_count'])\n",
    "        with open('{}/bin_{}.txt'.format(path_prime, i), 'w') as f:\n",
    "            f.write(template_filled)\n",
    "            combine_str+= ' Name{}={}/bin_{}.txt'.format(i,signal, i)\n",
    "    print(\"signal_total: \", signal_total, \" norm \", norm)\n",
    "    combine_str += ' > datacard_{}.txt\\n'.format(signal)\n",
    "    return combine_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_sys(row, string):\n",
    "    syses = sorted([1+row[string+\"_Up\"]/row['acceptance'], 1+row[string+\"_Down\"]/row['acceptance']])\n",
    "\n",
    "    syses_string = [\"{:.2f}\".format(sys) for sys in syses]\n",
    "    if syses_string[0]==\"1.00\" and syses_string[1]==\"1.00\": syses_string=\"1.00\"\n",
    "    else: syses_string = \"{}/{}\".format(*syses_string)\n",
    "    return syses_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = [164.0]\n",
    "accpt_df.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1890a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combine_sh = '#!bin/bash\\n'\n",
    "dbs = 0.5\n",
    "\n",
    "for fs_type in accpt_df.type.unique():\n",
    "    for mass in masses:\n",
    "        print(fs_type, mass)\n",
    "        for reg in ['SR1', 'SR2']:\n",
    "\n",
    "            if reg!='SR2': continue\n",
    "            #print(fs_type, mass, reg)f\n",
    "            combine_sh += make_card(reg, mass, dbs, fs_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/{}/make_combine.sh'.format(path, era), 'w') as f:\n",
    "    f.write(combine_sh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979cc88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for fs_type in accpt_df.type.unique():\n",
    "    for mass in [125, 150, 175, 200, 250, 300, 350]:\n",
    "        combine_sh_all_years ='combineCards.py '\n",
    "        i = 0\n",
    "        for era in [2016, 2017, 2018]:\n",
    "            for reg in ['SR1', 'SR2']:\n",
    "                combine_sh_all_years += \" Name{i}={era}/datacard_{reg}_{mass}_0p5_{fs_type}.txt\".format(i=i, era=era, reg=reg,mass=mass, \n",
    "                                                                                                        fs_type=fs_type.replace('(','-').replace(')','-'))\n",
    "                i+=1\n",
    "        combine_sh_all_years += ' > 201X/datacard_{}_0p5_{}.txt'.format(mass, fs_type.replace('(','-').replace(')','-'))\n",
    "        print(combine_sh_all_years)\n",
    "    \n",
    "for fs_type in accpt_df.type.unique():\n",
    "    for mass in [125, 150, 175, 200, 250, 300, 350]:\n",
    "        combine_sh_all_years ='combineCards.py '\n",
    "        i = 0\n",
    "        for era in [2016, 2017, 2018]:\n",
    "            for reg in ['SR1']:\n",
    "                combine_sh_all_years += \" Name{i}={era}/datacard_{reg}_{mass}_0p5_{fs_type}.txt\".format(i=i, era=era, reg=reg,mass=mass, \n",
    "                                                                                                        fs_type=fs_type.replace('(','-').replace(')','-'))\n",
    "                i+=1\n",
    "        combine_sh_all_years += ' > 201X/datacard_SR1_{}_0p5_{}.txt'.format(mass, fs_type.replace('(','-').replace(')','-'))\n",
    "        print(combine_sh_all_years)\n",
    "    \n",
    "for fs_type in accpt_df.type.unique():\n",
    "    for mass in [125, 150, 175, 200, 250, 300, 350]:\n",
    "        combine_sh_all_years ='combineCards.py '\n",
    "        i = 0\n",
    "        for era in [2016, 2017, 2018]:\n",
    "            for reg in ['SR2']:\n",
    "                combine_sh_all_years += \" Name{i}={era}/datacard_{reg}_{mass}_0p5_{fs_type}.txt\".format(i=i, era=era, reg=reg,mass=mass, \n",
    "                                                                                                        fs_type=fs_type.replace('(','-').replace(')','-'))\n",
    "                i+=1\n",
    "        combine_sh_all_years += ' > 201X/datacard_SR2_{}_0p5_{}.txt'.format(mass, fs_type.replace('(','-').replace(')','-'))\n",
    "        print(combine_sh_all_years)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881f4c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# era combine regions\n",
    "for fs_type in accpt_df.type.unique():\n",
    "    fs_string = fs_type.replace('(','-').replace(')','-')\n",
    "    for dbs in [0.5]:\n",
    "        dbsstring = str(dbs).replace('.', 'p')\n",
    "        for mass in [125, 150, 175, 200, 250, 300, 350]:\n",
    "            for era in [2016, 2017, 2018]:\n",
    "                combine_sh ='combineCards.py '\n",
    "                i = 0\n",
    "                for reg in ['SR1', 'SR2']:\n",
    "                    combine_sh += f\" Name{i}={era}/datacard_{reg}_{mass}_0p5_{fs_string}.txt\"\n",
    "                    i+=1\n",
    "                combine_sh += f' > {era}/datacard_{mass}_0p5_{fs_string}.txt'\n",
    "                print(combine_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440773b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_combine = '''#!/bin/sh\n",
    "#ulimit -s unlimited\n",
    "#set -e\n",
    "cd /afs/cern.ch/work/r/rymuelle/public/nanoAODzPrime/higgscombine/CMSSW_10_2_13/src\n",
    "export SCRAM_ARCH=slc7_amd64_gcc700\n",
    "source /cvmfs/cms.cern.ch/cmsset_default.sh\n",
    "eval `scramv1 runtime -sh`\n",
    "cd {path} \n",
    "\n",
    "combine -M AsymptoticLimits \"$1\"\n",
    "\n",
    "'''.format(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8420ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cp src/combine_scripts/* {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ded883",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/run_combine.sh'.format(path), 'w') as f:\n",
    "    f.write(run_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375285f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {path}/out\n",
    "!mkdir {path}/err\n",
    "!mkdir {path}/log\n",
    "\n",
    "!mkdir {path}/out/2016\n",
    "!mkdir {path}/err/2016\n",
    "!mkdir {path}/log/2016\n",
    "\n",
    "!mkdir {path}/out/2017\n",
    "!mkdir {path}/err/2017\n",
    "!mkdir {path}/log/2017\n",
    "\n",
    "\n",
    "!mkdir {path}/out/2018\n",
    "!mkdir {path}/err/2018\n",
    "!mkdir {path}/log/2018\n",
    "\n",
    "\n",
    "!mkdir {path}/out/201X\n",
    "!mkdir {path}/err/201X\n",
    "!mkdir {path}/log/201X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_submit submit_jobs_201X.sub\n",
    "condor_submit submit_jobs_2016.sub\n",
    "condor_submit submit_jobs_2017.sub\n",
    "condor_submit submit_jobs_2018.sub\n",
    "watch condor_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d6a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481361d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bff_12_1",
   "language": "python",
   "name": "bff_12_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
